{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l6LCQQYq9-5",
        "outputId": "9f60aa99-94f5-4c9f-fd36-522430d619f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrOBlU08Y6sU",
        "outputId": "8d8849a1-14aa-40c5-8c57-53b66e3890a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cusp-pytorch'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Total 185 (delta 0), reused 0 (delta 0), pack-reused 185\u001b[K\n",
            "Receiving objects: 100% (185/185), 9.44 MiB | 16.33 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "/content/cusp-pytorch\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/guillermogotre/cusp-pytorch/\n",
        "%cd cusp-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyAU4wAkY_r7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Custom modules\n",
        "from training.networks import VGG, module_no_grad\n",
        "import legacy\n",
        "from torch_utils import misc\n",
        "import dnnlib\n",
        "\n",
        "# GDrive authentication and Download\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLQw5Lsa_i37"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dir_list = []\n",
        "for dir in os.listdir('/content/gdrive/MyDrive/FGNET73/train'):\n",
        "  dir_list.append(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAQWytO-AfE-",
        "outputId": "ff85d09f-a7a7-4eb8-ca0a-f50e2c329fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82\n"
          ]
        }
      ],
      "source": [
        "print(len(dir_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKAOeJHkZD2W"
      },
      "outputs": [],
      "source": [
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEezEfb5ap2T"
      },
      "outputs": [],
      "source": [
        "FFHQ_RR_KEY = \"hrfae\" # Model trained on HRFAE dataset\n",
        "\n",
        "# Choose one from above\n",
        "KEY = FFHQ_RR_KEY \n",
        "\n",
        "# Config and GDrive ID\n",
        "configs = {\n",
        "    FFHQ_RR_KEY: dict(\n",
        "        side=224, \n",
        "        classes=(20,60))\n",
        "}\n",
        "\n",
        "# CUDA device\n",
        "device = torch.device('cuda',0)\n",
        "\n",
        "# Side of input images\n",
        "side = configs[KEY]['side']\n",
        "\n",
        "# Weights paths\n",
        "weights_path = \"/content/gdrive/MyDrive/CUSP_implement/network-snapshot-002408.pkl\"\n",
        "vgg_path = \"/content/gdrive/MyDrive/CUSP_implement/dex_imdb_wiki.caffemodel.pt\"\n",
        "\n",
        "data_dir = '/content/gdrive/MyDrive/FGNET64/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzUEnmFoE4jH"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path,vgg_path,device):        \n",
        "    with open(model_path,'rb') as f:\n",
        "        contents = legacy.load_network_pkl(f) # Pickles weights and source code\n",
        "    \n",
        "    # Get exponential movign average model\n",
        "    G_ema = contents['G_ema']\n",
        "    \n",
        "    # Load DEX VGG classifier\n",
        "    vgg = VGG()\n",
        "    vgg_state_dict = torch.load(vgg_path)\n",
        "    vgg_state_dict = {k.replace('-', '_'): v for k, v in vgg_state_dict.items()}\n",
        "    vgg.load_state_dict(vgg_state_dict)\n",
        "    module_no_grad(vgg) #!important\n",
        "\n",
        "    # Set classifier\n",
        "    G_ema.skip_grad_blur.model.classifier = vgg        \n",
        "    # No grad\n",
        "    G_ema = G_ema.to(device).eval().requires_grad_(False)\n",
        "    # No grad on VGG\n",
        "\n",
        "    return G_ema\n",
        "\n",
        "G_ema = load_model(weights_path, vgg_path, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40sKGiztbR6J"
      },
      "outputs": [],
      "source": [
        "def run_model(G, img, label, global_blur_val=None, mask_blur_val=None, return_msk = False):\n",
        "    # Tranform label to One Hot Encoding\n",
        "    cls = torch.nn.functional.one_hot(\n",
        "        torch.tensor(label), \n",
        "        num_classes=G.attr_map.fc0.init_args[0]\n",
        "    ).to(img.device)\n",
        "    \n",
        "    # Content encoder\n",
        "    _,c_out_skip = G.content_enc(img)\n",
        "    \n",
        "    # Style encodder\n",
        "    s_out = G.style_enc(img)[0].mean((2, 3))\n",
        "    \n",
        "    truncation_psi=1\n",
        "    truncation_cutoff=None\n",
        "    s_out = G.style_map(s_out, None, truncation_psi, truncation_cutoff)\n",
        "    \n",
        "    # age mapping\n",
        "    a_out = G.attr_map(cls.to(s_out.device), None, truncation_psi, truncation_cutoff)\n",
        "\n",
        "    # Style mapping and Age mapping are interleaved for the corresponding \n",
        "    # weight demodulation modules\n",
        "    w = G.__interleave_attr_style__(a_out, s_out)\n",
        "\n",
        "    # Global blur\n",
        "    for i,(f,_) in enumerate(zip(G.skip_transf, c_out_skip)):\n",
        "        if f is not None:\n",
        "            c_out_skip[i] = G._batch_blur(c_out_skip[i], blur_val = global_blur_val)\n",
        "    \n",
        "    # Masked blur\n",
        "    cam = G.skip_grad_blur(img.float())\n",
        "    msk = cam\n",
        "    for i, (f, c) in enumerate(zip(G.skip_transf, c_out_skip)):\n",
        "        if f is not None:\n",
        "            im_size = c.size(-1)\n",
        "            blur_c = G._batch_blur(c, blur_val= mask_blur_val)\n",
        "            if msk.size(2) != im_size:\n",
        "                msk = F.interpolate(msk,size=(im_size,im_size), mode='area')\n",
        "            merged_c = c * msk + blur_c * (1 - msk)\n",
        "            c_out_skip[i] = merged_c\n",
        "            \n",
        "\n",
        "    # Decoder\n",
        "    img_out = G.image_dec(c_out_skip, w)\n",
        "\n",
        "    if return_msk:\n",
        "        to_return = (img_out,msk,cam) if G.learn_mask is not None else (img_out,None,None)\n",
        "    else:\n",
        "        to_return = img_out\n",
        "    \n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4CHqTj_FLTM"
      },
      "outputs": [],
      "source": [
        "# Transform tensor to uint8 image\n",
        "def to_uint8(im_tensor):\n",
        "    im_tensor = (im_tensor.detach().cpu().numpy().transpose((1,2,0))+1)*(256/2)\n",
        "    im_tensor = np.clip(im_tensor,0,255).astype(np.uint8)\n",
        "    return im_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXhoJlZ1BCYf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "for dir in dir_list:\n",
        "  img_count = sum(len(files) for _, _, files in os.walk(os.path.join(data_dir, dir)))\n",
        "  steps = 30 // img_count\n",
        "  sample_images_path= os.path.join(data_dir, dir) \n",
        "\n",
        "  # Read image filenames\n",
        "  filenames_batch = [\n",
        "        os.path.join(sample_images_path,f) \n",
        "        for  f in next(iter(os.walk(sample_images_path)))[2] \n",
        "        if f[-4:] == '.JPG'\n",
        "      ]\n",
        "\n",
        "  # Read images\n",
        "  imgs = [np.array(PIL.Image.open(f).resize((side,side)).convert(\"RGB\"),dtype=np.float32).transpose((2,0,1)) for f in filenames_batch]\n",
        "  # Transform to tensors\n",
        "  im_in_tensor = (torch.tensor(np.array(imgs))/256*2-1).cuda() # Values {-1,1}\n",
        "\n",
        "  # Repeat images N times\n",
        "  n_images = im_in_tensor.shape[0]\n",
        "  im_in_tensor_exp = im_in_tensor[:,None].expand([n_images,steps,*im_in_tensor.shape[1:]]).reshape([-1,*im_in_tensor.shape[1:]])\n",
        "  # Labels range for examples generation\n",
        "  data_labels_range = configs[KEY]['classes']\n",
        "  # Define target ages\n",
        "  labels_exp = torch.tensor(np.repeat(np.linspace(*data_labels_range,steps,dtype=int)[:,None],n_images,1).T.reshape(-1))\n",
        "\n",
        "  \n",
        "  batch_size = img_count\n",
        "  # Run model\n",
        "  im_out_tensor_exp = torch.concat([run_model(\n",
        "      G_ema,\n",
        "      mini_im,\n",
        "      mini_label,\n",
        "      global_blur_val=0, # CUSP global blur\n",
        "      mask_blur_val=0)   # CUSP masked blur\n",
        "      for mini_im, mini_label\n",
        "      in zip(\n",
        "          im_in_tensor_exp.split(batch_size),\n",
        "          labels_exp.split(batch_size)\n",
        "      )])\n",
        "  # Transform to [batch_size, N_ages, W, H , C]\n",
        "  im_out_tensor = im_out_tensor_exp.reshape([-1,steps,*im_out_tensor_exp.shape[1:]])\n",
        "\n",
        "  for fname, im_in, im_out, age_labels in zip(\n",
        "        filenames_batch,im_in_tensor,im_out_tensor, \n",
        "        labels_exp.numpy().reshape(-1,steps)\n",
        "        ):\n",
        "    age_labels = [i for i in age_labels]\n",
        "    image_name = fname.split('/')[-1]\n",
        "    # For every [input,step...]\n",
        "    for im,l in zip(im_out,age_labels):\n",
        "        im = Image.fromarray(to_uint8(im)).resize((250,250))\n",
        "        saved_image_name = image_name.split('.')[0] + f'_GAN_{str(l)}'+'.JPG'\n",
        "        im.save(os.path.join(sample_images_path, saved_image_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fDiXZUzEfK7"
      },
      "source": [
        "--------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
